{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is for an image segmentation task. It defines a model, imports the libraries, and structures training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import warnings\n",
    "from keras import optimizers, regularizers\n",
    "\n",
    "from keras.layers import Input, TimeDistributed, GaussianNoise, GaussianDropout, SeparableConv2D\n",
    "from keras import layers\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Permute\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten, Multiply, Add, Concatenate, Maximum, Subtract, Average\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers import merge, Convolution2D, UpSampling2D,Deconvolution2D,AtrousConvolution2D,ZeroPadding2D,multiply,Conv2DTranspose\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import losses\n",
    "import keras.backend as K\n",
    "from keras.utils import conv_utils\n",
    "from keras.engine.topology import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I added a data generator. The goal of this is to create transformations of the original data set in a way that teaches the model a wider array of patterns. This increases generalization capacity of the model. I had enormous trouble adding the elastic transforms into the Keras data generator, but if ran separately on data sets with lot's of variance in shape of objects, it's a great way to increase the model capacity, particularly near object boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed = seed\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "data_gen_args = dict(\n",
    "                         #zca_whitening=True,\n",
    "                         #zca_epsilon=1e-5,\n",
    "                         #featurewise_std_normalization=True,\n",
    "                         rotation_range=0.2,\n",
    "                         width_shift_range=0.33,\n",
    "                         height_shift_range=0.33,\n",
    "                         #channel_shift_range=0.0001,\n",
    "                         shear_range=0.1,\n",
    "                         zoom_range=0.3,\n",
    "                         horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         fill_mode='reflect')  #use 'constant'??\n",
    "# Train data, provide the same seed and keyword arguments to the fit and flow methods\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# input generator with standardization on\n",
    "def elastic_transform(X):\n",
    "    \n",
    "    random_state = None\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "    sigma = .001\n",
    "    alpha = .001\n",
    "    shape = X.shape\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dz = np.zeros_like(dx)\n",
    " \n",
    "    x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]))\n",
    "    print x.shape\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
    " \n",
    "    distored_image = map_coordinates(X, indices, order=1, mode='reflect')\n",
    "    return distored_image.reshape(X.shape)\n",
    "\n",
    "X_datagen = ImageDataGenerator(**data_gen_args)\n",
    "Y_datagen = ImageDataGenerator(**data_gen_args)\n",
    "X_datagen.fit(np.asarray(X_train), augment=True, seed=seed)\n",
    "Y_datagen.fit(np.asarray(Y_train), augment=True, seed=seed)\n",
    "\n",
    "X_train_augmented = X_datagen.flow(np.asarray(X_train), batch_size=4, shuffle=True, seed=seed)\n",
    "Y_train_augmented = Y_datagen.flow(np.asarray(Y_train), batch_size=4, shuffle=True, seed=seed)\n",
    "\n",
    "import itertools \n",
    "train_generator = itertools.izip(X_train_augmented, Y_train_augmented) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, a few loss functions for image segmentation are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
    "\n",
    "\n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "\n",
    "def jacard_coef_loss(y_true, y_pred):\n",
    "    return -jacard_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the custom layers I used for models are shown. I used squeeze excite blocks, and a few variants of grouped and separable convolution blocks. The code for the superseparable grouped convolution block only seems to work with certain versions of Keras (I couldn't run it with Keras. 2.1 or up), but, it is basically a grouped convolution, borrowed from a resnext model, with a separable convolution layer instead of a regular convolution layer for the depthwise convolution step. The edis layer is meant to mimic the multiplicative integration method used in some recurrent neural network models. This should allow for self gating asnd an attention like mechanism, like the swish activation function, and also allow the merge to take a very flexble form. In theory, it can be used any time there is a merge operation such as in an lstm unit, or here, as part of a gating operation. For what it's worth, it didn't seem to improve things for this particular model. Perhaps the expressiveness it gives a recurrent model is already occurring for a convolutional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squeeze_excite_block1(input, ratio=8):\n",
    "    ''' Create a squeeze-excite block usually added right before the add merge\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "        k: width factor\n",
    "    Returns: a keras tensor\n",
    "    '''\n",
    "    init = input\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    filters = init._keras_shape[channel_axis]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='elu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = GaussianDropout(.01)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        se = Permute((3, 1, 2))(se)\n",
    "\n",
    "    x = multiply([init, se])\n",
    "    return \n",
    "\n",
    "def __grouped_convolution_block(input, grouped_channels, cardinality, strides, weight_decay=5e-4):\n",
    "    ''' Adds a grouped convolution block. It is an equivalent block from the paper\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        grouped_channels: grouped number of filters\n",
    "        cardinality: cardinality factor describing the number of groups\n",
    "        strides: performs strided convolution for downscaling if > 1\n",
    "        weight_decay: weight decay term\n",
    "    Returns: a keras tensor\n",
    "    '''\n",
    "    init = input\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    group_list = []\n",
    "\n",
    "    if cardinality == 1:\n",
    "        # with cardinality 1, it is a standard convolution\n",
    "        x = Conv2D(grouped_channels, (3, 3), padding='same', use_bias=False, strides=(strides, strides),\n",
    "                   kernel_initializer='he_normal')(init)\n",
    "        x = BatchNormalization(axis=channel_axis)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    for c in range(cardinality):\n",
    "        x = Lambda(lambda z: z[:, :, :, c * grouped_channels:(c + 1) * grouped_channels]\n",
    "        if K.image_data_format() == 'channels_last' else\n",
    "        lambda z: z[:, c * grouped_channels:(c + 1) * grouped_channels, :, :])(input)\n",
    "\n",
    "        x = Conv2D(grouped_channels, (3, 3), padding='same', use_bias=False, strides=(strides, strides),\n",
    "                   kernel_initializer='he_normal')(x)\n",
    "\n",
    "        group_list.append(x)\n",
    "\n",
    "    group_merge = concatenate(group_list, axis=channel_axis)\n",
    "    x = BatchNormalization(axis=channel_axis)(group_merge)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "def superseparable_convolution(input, width1, grouped_channels, cardinality, name):\n",
    "    ''' Adds a grouped convolution block. It is an equivalent block from the paper\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        grouped_channels: grouped number of filters\n",
    "        cardinality: cardinality factor describing the number of groups\n",
    "        strides: performs strided convolution for downscaling if > 1\n",
    "        weight_decay: weight decay term\n",
    "    Returns: a keras tensor\n",
    "    '''\n",
    "    init = input\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    group_list = []\n",
    "\n",
    "    if cardinality == 1:\n",
    "        # with cardinality 1, it is a standard convolution\n",
    "        x = Conv2D(grouped_channels, (width1, width1), padding='same', use_bias=False, strides=(strides, strides),\n",
    "                   kernel_initializer='he_normal')(init)\n",
    "        x = BatchNormalization(axis=channel_axis)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    for c in range(cardinality):   #cardinality should be group 2 or 3, alternating, otherwise no information is exchanged\n",
    "        x = Lambda(lambda z: z[:, :, :, c * grouped_channels:(c + 1) * grouped_channels]\n",
    "        if K.image_data_format() == 'channels_last' else\n",
    "        lambda z: z[:, c * grouped_channels:(c + 1) * grouped_channels, :, :])(input)\n",
    "\n",
    "        x = SeparableConv2D(grouped_channels, (width1, width1), padding='same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', use_bias=False,\n",
    "                   kernel_initializer='he_normal')(x)\n",
    "\n",
    "        group_list.append(x)\n",
    "\n",
    "    group_merge = concatenate(group_list, axis=channel_axis)\n",
    "    x = BatchNormalization(axis=channel_axis)(group_merge)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "    \n",
    "def grouped_convolution(y, nb_channels, cardinality, _strides):\n",
    "        # when `cardinality` == 1 this is just a standard convolution\n",
    "        if cardinality == 1:\n",
    "            return layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "        \n",
    "        assert not nb_channels % cardinality\n",
    "        _d = nb_channels // cardinality\n",
    "\n",
    "        # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "        # and convolutions are separately performed within each group\n",
    "        groups = []\n",
    "        for j in range(cardinality):\n",
    "            group = layers.Lambda(lambda z: z[:, :, :, j * _d:j * _d + _d])(y)\n",
    "            groups.append(layers.SeparableConv2D(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
    "            \n",
    "        # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "        y = layers.concatenate(groups)\n",
    "\n",
    "        return \n",
    "    \n",
    "def edis(inputs):\n",
    "    s = inputs[0]*inputs[1] + .5*inputs[0] + .5*inputs[1] +.25\n",
    "    \n",
    "    return s\n",
    "\n",
    "def euclid_dist(v):\n",
    "    return (v[0] - v[1])**2\n",
    "\n",
    "def out_shape(shapes):\n",
    "    return shapes[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a few of the blocks. These are combinations of the above layers, with different merge functions. Basically, a mix of resnet and densenet architectures, where concatenation, adding, or the multiplicative integration/edis layers are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lblock6(x, filters, width1, name):\n",
    "    Train1 = True\n",
    "    x1 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'a')(x)   #filters=grouped_channels, caridnality=2\n",
    "    x1 = BatchNormalization(momentum=.9)(x1)\n",
    "    \n",
    "    x2 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'b')(x1) \n",
    "    x2 = BatchNormalization(momentum=.9)(x2)\n",
    "    #x2 = grouped_convolution(x2, filters, 2, 1)\n",
    "    x3 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'c')(x1) \n",
    "    x3 = BatchNormalization(momentum=.9)(x3)\n",
    "    x3 = Activation('sigmoid')(x3)\n",
    "    \n",
    "    x4 = Lambda(edis)([x3, x2])\n",
    "    x4 = Lambda(zerolower2)(x4)\n",
    "    x4 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'd')(x4)   #filters=g\n",
    "    x4 = squeeze_excite_block(x4, name, ratio=8)\n",
    "    x4 = Add()([x1, x4])\n",
    "    x4 = Activation('elu')(x4)\n",
    "    return x4   \n",
    "\n",
    "def lblock5(x, filters, width1, name):\n",
    "    Train1 = True\n",
    "    x1 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'a')(x)   #filters=grouped_channels, caridnality=2\n",
    "    x1 = BatchNormalization(momentum=.9)(x1)\n",
    "    \n",
    "    x2 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'b')(x1) \n",
    "    x2 = BatchNormalization(momentum=.9)(x2)\n",
    "    #x2 = grouped_convolution(x2, filters, 2, 1)\n",
    "    x3 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'c')(x1) \n",
    "    x3 = BatchNormalization(momentum=.9)(x3)\n",
    "    x3 = Activation('sigmoid')(x3)\n",
    "    \n",
    "    x4 = Lambda(edis)([x3, x2])\n",
    "    \n",
    "    x4 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'd')(x4)   #filters=g\n",
    "    x4 = squeeze_excite_block(x4, name, ratio=8)\n",
    "    x4 = Add()([x1, x4])\n",
    "    x4 = Activation('elu')(x4)\n",
    "    return x4   \n",
    "\n",
    "def lblock5a(x, filters, width1, name):\n",
    "    Train1 = True\n",
    "    x1 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'a')(x)   #filters=grouped_channels, caridnality=2\n",
    "    x1 = BatchNormalization(momentum=.9)(x1)\n",
    "    \n",
    "    x2 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'b')(x1) \n",
    "    x2 = BatchNormalization(momentum=.9)(x2)\n",
    "    \n",
    "    x3 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'c')(x1) \n",
    "    x3 = BatchNormalization(momentum=.9)(x3)\n",
    "    x3 = Activation('sigmoid')(x3)\n",
    "    \n",
    "    x4 = Lambda(edis)([x3, x2])\n",
    "    x4 = Lambda(zerolower1)(x4)\n",
    "    x4 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'd')(x4) \n",
    "    x4 = BatchNormalization(momentum=.9)(x4)\n",
    "    x4 = squeeze_excite_block(x4, name, ratio=8)\n",
    "    x4 = Add()([x1, x4])\n",
    "    x4 = Activation('elu')(x4)\n",
    "    return x4\n",
    "\n",
    "def lblock5c(x, filters, width1, name):\n",
    "    Train1 = True\n",
    "    x1 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'a')(x)   #filters=grouped_channels, caridnality=2\n",
    "    x1 = BatchNormalization(momentum=.9)(x1)\n",
    "    \n",
    "    x2 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'b')(x1) \n",
    "    x2 = BatchNormalization(momentum=.9)(x2)\n",
    "    \n",
    "    x3 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'c')(x1) \n",
    "    x3 = BatchNormalization(momentum=.9)(x3)\n",
    "    x3 = Activation('sigmoid')(x3)\n",
    "    \n",
    "    x4 = Lambda(edis)([x3, x2])\n",
    "    x4 = Lambda(zerolower2)(x4)\n",
    "    x4 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'd')(x4) \n",
    "    x4 = BatchNormalization(momentum=.9)(x4)\n",
    "    x4 = squeeze_excite_block(x4, name, ratio=8)\n",
    "    x4 = Add()([x1, x4])\n",
    "    x4 = Activation('elu')(x4)\n",
    "    return x4    \n",
    "    \n",
    "def lblock2(x, filters, width1, name):\n",
    "    Train1 = True\n",
    "    x1 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'a')(x)   #filters=grouped_channels, caridnality=2\n",
    "    x1 = BatchNormalization(momentum=.9)(x1)\n",
    "    \n",
    "    x2 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'b')(x1) \n",
    "    x2 = BatchNormalization(momentum=.9)(x2)\n",
    "    \n",
    "    x3 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'c')(x1) \n",
    "    x3 = BatchNormalization(momentum=.9)(x3)\n",
    "    x3 = Activation('sigmoid')(x3)\n",
    "    \n",
    "    x4 = Multiply()([x2, x3])\n",
    "    \n",
    "    x4 = SeparableConv2D(filters, width1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = name + 'd')(x4)   #filters=g\n",
    "    x4 = squeeze_excite_block(x4, name, ratio=8)\n",
    "    x4 = Add()([x1, x4])\n",
    "    x4 = Activation('elu')(x4)\n",
    "    return x4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, two sample models are shown. The concatenated pooling layer was wildly inefficient both in terms of computation time and overfitting. The idea was to make the pooling layer more flexible by allowing it to learn how to weight max pooling and average pooling operations in a way that better described the data, but at least on a data set towards the small side in terms of the number of samples, it was consistently difficult to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lb_unet12():\n",
    "    Input1 = Input(shape=(256,256,3))\n",
    "    x = GaussianDropout(.02)(Input1)\n",
    "    x1 = lblock5(x, 24, 3, 'a4')\n",
    "    x1 = lblock5(x1, 24, 3, 'a1')\n",
    "    x2 = concatenated_pooling2(x1)  #128\n",
    "   \n",
    "    x3 = lblock5(x2, 48, 3, 'cbc')\n",
    "    #x3 = lblock(x2, 32,  'cbcc')\n",
    "   \n",
    "    x4 = concatenated_pooling2(x3)  #64\n",
    "  \n",
    "    x5 = lblock5(x4, 96, 3, 'c')\n",
    "    x6 = concatenated_pooling2(x5) #32\n",
    "    \n",
    "    x7a = lblock5(x6, 96, 1, 'k')\n",
    "    \n",
    "    #x7 = lblock(x6, 64, 13, 'd')\n",
    "    x8 = lblock5(x6, 96, 7, 'e')\n",
    "    x9 = lblock5(x6, 96, 15,  'f')\n",
    "    \n",
    "    \n",
    "    x10 = Concatenate(axis=3)([x7a, x8, x9])\n",
    "    x11 = SeparableConv2D(96, 1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = 'atrsum')(x10)\n",
    "    x11 = BatchNormalization(momentum=.9)(x11)\n",
    "    x11 = activ3(x11)\n",
    "    x11 = GaussianDropout(.02)(x11)\n",
    "    x12 = Concatenate(axis=3)([x11,x6])\n",
    "    x13 = UpSampling2D((2,2))(x12) #64\n",
    "    x14 = lblock5(x13, 48, 3, 'g')\n",
    "    \n",
    "    x15 = Concatenate(axis=3)([x14,x4]) \n",
    "    x16 = UpSampling2D((2,2))(x15) #128\n",
    "    x17 = lblock5(x16, 48, 3, 'h')\n",
    "    \n",
    "    x18 = Concatenate(axis=3)([x17,x2]) \n",
    "    x19 = UpSampling2D((2,2))(x18) #128\n",
    "    x20 = lblock5(x19, 48, 3, 'i')\n",
    "    x20 = GaussianDropout(.01)(x20)\n",
    "    #x21 = UpSampling2D((2,2))(x20) #128\n",
    "    x22 = lblock5(x20, 24, 3, 'j')\n",
    "   \n",
    "    x22 = lblock5(x22, 16, 3, 'jj')\n",
    "    xout = Conv2D(1,1, kernel_initializer=dice_coef_loss, padding='same', activation='sigmoid')(x22)\n",
    "    model=Model(Input1, xout)\n",
    "    return model\n",
    "modelt6.compile(optimizer=ADM, loss=dice_coef_loss, metrics=[binary_crossentropy, 'accuracy'])\n",
    "modelt6 = lb_unet12()\n",
    "ADM = optimizers.Adam(lr=0.0059, beta_1=0.9, beta_2=0.999, decay=0.0005, clipnorm=.9)\n",
    "modelt6.fit_generator(train_generator, steps_per_epoch=600, epochs=3, shuffle=True)\n",
    "modelt6.save('lb_unet12t_2.h5')  #2 epoches .0985 bince\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelt6.compile(optimizer=ADM, loss = dice_coef_loss, metrics=[binary_crossentropy, dice_coef, 'accuracy'])\n",
    "modelt6.save('lb_unet12t_1.h5')\n",
    "\n",
    "\n",
    "def lb_unet11():\n",
    "    Input1 = Input(shape=(256,256,3))\n",
    "    x = GaussianDropout(.02)(Input1)\n",
    "    x1 = lblock5(x, 24, 3, 'a4')\n",
    "    x1 = lblock5(x1, 24, 3, 'a1')\n",
    "    x2 = concatenated_pooling2(x1)  #128\n",
    "   \n",
    "    x3 = lblock6a(x2, 48, 3, 'cbc')\n",
    "    #x3 = lblock(x2, 32,  'cbcc')\n",
    "   \n",
    "    x4 = concatenated_pooling2(x3)  #64\n",
    "  \n",
    "    x5 = lblock6a(x4, 96, 3, 'c')\n",
    "    x6 = concatenated_pooling2(x5) #32\n",
    "    \n",
    "    x7a = lblock6a(x6, 96, 1, 'k')\n",
    "    \n",
    "    #x7 = lblock(x6, 64, 13, 'd')\n",
    "    x8 = lblock6a(x6, 96, 7, 'e')\n",
    "    x9 = lblock6a(x6, 96, 15,  'f')\n",
    "    \n",
    "    \n",
    "    x10 = Concatenate(axis=3)([x7a, x8, x9])\n",
    "    x11 = SeparableConv2D(96, 1, padding = 'same', depthwise_initializer = 'he_normal', pointwise_initializer = 'he_normal', name = 'atrsum')(x10)\n",
    "    x11 = BatchNormalization(momentum=.9)(x11)\n",
    "    x11 = activ3(x11)\n",
    "    x11 = GaussianDropout(.02)(x11)\n",
    "    x12 = Concatenate(axis=3)([x11,x6])\n",
    "    x13 = UpSampling2D((2,2))(x12) #64\n",
    "    x14 = lblock6a(x13, 48, 3, 'g')\n",
    "    \n",
    "    x15 = Concatenate(axis=3)([x14,x4]) \n",
    "    x16 = UpSampling2D((2,2))(x15) #128\n",
    "    x17 = lblock6a(x16, 48, 3, 'h')\n",
    "    \n",
    "    x18 = Concatenate(axis=3)([x17,x2]) \n",
    "    x19 = UpSampling2D((2,2))(x18) #128\n",
    "    x20 = lblock6a(x19, 48, 3, 'i')\n",
    "    x20 = GaussianDropout(.01)(x20)\n",
    "    #x21 = UpSampling2D((2,2))(x20) #128\n",
    "    x22 = lblock5(x20, 24, 3, 'j')\n",
    "   \n",
    "    x22 = lblock5(x22, 16, 3, 'jj')\n",
    "    xout = Conv2D(1,1, kernel_initializer='he_normal', padding='same', activation='sigmoid')(x22)\n",
    "    model=Model(Input1, xout)\n",
    "    return model\n",
    "modelt6.compile(optimizer=ADM, loss='binary_crossentropy', metrics=[binary_crossentropy, 'accuracy'])\n",
    "modelt6 = lb_unet11()\n",
    "ADM = optimizers.Adam(lr=0.0099, beta_1=0.9, beta_2=0.999, decay=0.0005, clipnorm=.9)\n",
    "modelt6.fit_generator(train_generator, steps_per_epoch=600, epochs=2, shuffle=True)\n",
    "modelt6.save('lb_unet11t_1.h5')  #2 epoches .0985 bince\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
