# Prediction-Models

This project is for a few custom predictive models I set up in the past. Most of them are pretty raw or were used as parts of other projects. For a much cleaner presentation of topics, please go to my 'Projects' branch. Where appropriate/feasible, I would like to add in supporting functions and some examples of the models performing on data as a future task. There are a few models contained within this project:

1) There are a minimum description length random forest and similarity forest models. The difference between these models and regular random forest and similarity forest models, is that the votes of the individual trees are not equal. Specifically, they are weighted using the minimum description length principle, which, when applied to decision tree based models, says that if two trees have similar training accuracy, then the tree with less depth is likely to be the more accurate on data it has not been trained on. There are accuracy weighted trees, where each tree's vote is weighted by the training accuracy, but these tend to overfit, while capping the maximum tree length to an arbitrarily low value tends to increase error due to bias. One way to approach this issue is to include both training accuracy and tree length as inputs to the weight of each tree's vote on test data. This adds two parameters to each model, and cross validation can be used to set the ratio at which accuracy and tree length parameters are each weighted. Since similarity forests obtain second order statistics, while random forests obtain first order statistics, the models can be complementary when used together.


2) There are also neural network models, written in Keras+tensorflow and mxnet. Some of these are written for R, and some with Python depending upon the preprocessing needed and other considerations. These models include:

- Wavenet:  This model is meant for predicting time series, not building sequences of speech as in the original paper.  Here, the gating is changed to x*sigmoid(x). The gating function for these is slightly different than in the original paper. This helps ensure that back propagation can still correct errors, since the derivative of x*sigmoid(x) is simply sigmoid(x) + x*sigmoid(x)*(1-sigmoid(x)). That first term in the derivative makes error propagation easier, ensuring weights are accurately updated. This gating convention is borrowed from the paper,  "Swish, A self generated Activation Function". This is written in Python, using Keras.

- Causal 1D model with separable convolutions: This model is also meant for forecasting. It combines causal convolutions, which prevents model training bias by preventing convolutions from accessing data from future time points with respect to the convolution, and instead of dilated convolutions, it uses larger convolution window lengths to enable translational invariance in a reasonable amount of layers. To prevent overfitting, separable convolutions (borrowed from Xception models) are used, as this decreases the number of parameters significantly, while still allowing function flexibility in terms of models. This is written in Python, using Mxnet.
